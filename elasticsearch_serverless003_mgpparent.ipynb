{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m pip install elasticsearch-serverless\n",
    "\n",
    "# If your application uses async/await in Python you can install with the async extra\n",
    "# python -m pip install elasticsearch_serverless[async]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test api key=amhNeGFwQUJ3THRlT245XzhDWVE6ZU8yMlpEdlBUMzJWam9DQk51ZE1rdw==\n",
    "# https://a965bb4a9ed8459d9ff14b43e9a4d034.es.us-east-1.aws.elastic.cloud:443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_serverless import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\n",
    "  \"https://a965bb4a9ed8459d9ff14b43e9a4d034.es.us-east-1.aws.elastic.cloud:443\",\n",
    "  api_key=\"amhNeGFwQUJ3THRlT245XzhDWVE6ZU8yMlpEdlBUMzJWam9DQk51ZE1rdw==\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mkaley\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMGP New\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmgp_parentwritein_22_23_largeformat.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m dfparent \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(parent)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(dfparent\u001b[38;5;241m.\u001b[39minfo())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "parent = r'C:\\Users\\kaley\\OneDrive\\Documents\\MGP New\\mgp_parentwritein_22_23_largeformat.csv'\n",
    "\n",
    "dfparent = pd.read_csv(parent)\n",
    "\n",
    "print(dfparent.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfutterances.info())  \n",
    "print(dfutterances.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "  { \"index\": { \"index\": \"books\", \"_id\": \"9780553351927\"}},\n",
    "  {\"name\": \"Snow Crash\", \"author\": \"Neal Stephenson\", \"release_date\": \"1992-06-01\", \"page_count\": 470},\n",
    "  { \"index\": { \"_index\": \"books\", \"_id\": \"9780441017225\"}},\n",
    "  {\"name\": \"Revelation Space\", \"author\": \"Alastair Reynolds\", \"release_date\": \"2000-03-15\", \"page_count\": 585},\n",
    "  { \"index\": { \"_index\": \"books\", \"_id\": \"9780451524935\"}},\n",
    "  {\"name\": \"1984\", \"author\": \"George Orwell\", \"release_date\": \"1985-06-01\", \"page_count\": 328},\n",
    "  { \"index\": { \"_index\": \"books\", \"_id\": \"9781451673319\"}},\n",
    "  {\"name\": \"Fahrenheit 451\", \"author\": \"Ray Bradbury\", \"release_date\": \"1953-10-15\", \"page_count\": 227},\n",
    "  { \"index\": { \"_index\": \"books\", \"_id\": \"9780060850524\"}},\n",
    "  {\"name\": \"Brave New World\", \"author\": \"Aldous Huxley\", \"release_date\": \"1932-06-01\", \"page_count\": 268},\n",
    "  { \"index\": { \"_index\": \"books\", \"_id\": \"9780385490818\"}},\n",
    "  {\"name\": \"The Handmaid's Tale\", \"author\": \"Margaret Atwood\", \"release_date\": \"1985-06-01\", \"page_count\": 311},\n",
    "]\n",
    "\n",
    "client.bulk(operations=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def count_records_in_csv(file_path):\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        record_count = sum(1 for row in reader) - 1  # Subtracting 1 to exclude the header row\n",
    "    return record_count\n",
    "\n",
    "# Example usage\n",
    "file_path = 'path/to/your/file.csv'\n",
    "print(f'The number of records in the CSV file is: {count_records_in_csv(file_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_dataframe_to_csv(df, chunk_size, output_prefix):\n",
    "    # Calculate the number of chunks needed\n",
    "    num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size\n",
    "        chunk = df.iloc[start:end]\n",
    "        chunk.to_csv(f'{output_prefix}_{i+1}.csv', index=False)\n",
    "\n",
    "\n",
    "# Split the DataFrame into CSV files with 300,000 records each\n",
    "split_dataframe_to_csv(dfutterances, 400000, r'C:\\Users\\kaley\\OneDrive\\Documents\\NPR Data\\utterances')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "def create_index(es, index_name):\n",
    "    # Define index settings and mappings if needed\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"field1\": {\"type\": \"text\"},\n",
    "                \"field2\": {\"type\": \"keyword\"},\n",
    "                \"field3\": {\"type\": \"date\"},\n",
    "                # Add other field mappings as necessary\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es.indices.create(index=index_name, body=index_settings, ignore=400)\n",
    "\n",
    "def bulk_upload_csv_to_elasticsearch(es, index_name, csv_files):\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        records = df.to_dict(orient='records')\n",
    "        actions = [\n",
    "            {\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": record\n",
    "            }\n",
    "            for record in records\n",
    "        ]\n",
    "        helpers.bulk(es, actions)\n",
    "        print(f'Successfully uploaded {file} to {index_name}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the index (if it doesn't already exist)\n",
    "index_name = 'npr001'\n",
    "create_index(client, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of CSV files to upload\n",
    "csv_files = [\n",
    "    r'C:\\Users\\kaley\\OneDrive\\Documents\\NPR Data\\utterances_1.csv',\n",
    "    r'C:\\Users\\kaley\\OneDrive\\Documents\\NPR Data\\utterances_2.csv',\n",
    "    r'C:\\Users\\kaley\\OneDrive\\Documents\\NPR Data\\utterances_3.csv',\n",
    "    r'C:\\Users\\kaley\\OneDrive\\Documents\\NPR Data\\utterances_4.csv',\n",
    "   r'C:\\Users\\kaley\\OneDrive\\Documents\\NPR Data\\utterances_5.csv',\n",
    "    r'C:\\Users\\kaley\\OneDrive\\Documents\\NPR Data\\utterances_6.csv',\n",
    "   r'C:\\Users\\kaley\\OneDrive\\Documents\\NPR Data\\utterances_7.csv'\n",
    "]\n",
    "\n",
    "#Upload CSV files to Elasticsearch\n",
    "bulk_upload_csv_to_elasticsearch(client, index_name, csv_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\n",
    "    r'C:\\Users\\kaley\\OneDrive\\Documents\\NPR Data\\utterances_1.csv']\n",
    "\n",
    "#Upload CSV files to Elasticsearch\n",
    "bulk_upload_csv_to_elasticsearch(client, index_name, csv_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
